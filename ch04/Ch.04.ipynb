{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 신경망 학습\n",
    "\n",
    "학습 = 가중치 매개변수를 최적화\n",
    "그 지표는? 손실함수. (손실함수의 최소화.)\n",
    "Gradient Descent 를 배울 것이다. \n",
    "\n",
    "## 4.1 데이터에서 학습한다!\n",
    "\n",
    "실제 신경망에서 매개변수 (a,w,b)는 수천 수만개다. \n",
    "2장의 퍼셉트론도 선형 분리 가능한 문제는 유한 번 학습을 통해 풀 수 있다. (퍼셉트론 수렴 정리, perceptrain convergence theorem)\n",
    "하지만 비선형 문제는 불가하다. \n",
    "\n",
    "### 4.1.1 데이터 주도 학습\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/2137CC4A593E64A224\">\n",
    "\n",
    "단순히 사람이 패턴을 파악해 모델을 설계할 수 없다. 데이터의 특징(feature)을 추출하고, 이를 기계학습 시켜야 한다. \n",
    "\n",
    "특징(feature)이란? 입력 데이터에서 본질적인 데이터를 정확하게 추출할 수 있도록 설계된 변환기이다. \n",
    "예를 들어 컴퓨터 비전 분야에선 이미지를 벡터로 기술하고 이를 SIFT, SURF, HOG 등의 feature로 뽑는다. 그리고 SVM, KNN등의 기계학습을 돌린다. \n",
    "주의할 것은, 문제에 적합한 특징을 설계하는 것은 사람이 하는 일이라는 것이다. \n",
    "\n",
    "그런데 신경망(딥러닝)은 이런 것도 필요없다. 있는 그대로를 학습한다. feature까지도 기계가 학습하는 것이다. (종단간 기계학습, end-to-end machine learning: 사람의 개입없이 입력~출력까지 얻는다.)\n",
    "\n",
    "### 4.1.2 훈련 데이터와 시험 데이터\n",
    "\n",
    "traning data / test data 분리. \n",
    "왜 나누는가? 우리는 범용적인 모델을 원하기 떄문. 아직 보지 못한 데이터도 잘 풀 수 있는지 evaluation이 필요하다. \n",
    "overfitting / underfitting 문제. \n",
    "\n",
    "## 4.2 손실 함수\n",
    "\n",
    "주로 \n",
    "- 평균 제곱 오차 (Mean Square Error)\n",
    "- 교차 엔트로피 오차 \n",
    "\n",
    "를 이용한다. \n",
    "\n",
    "### 4.2.1 평균 제곱 오차\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/241A674E593E6CB206\">\n",
    "yk 는 추정값. (y hat), tk는 정답 레이블 (y), k는 데이터의 차원 수 이다. \n",
    "\n",
    "t = [0,0,1,0,...] 로 나와있는데, 이는 one-hot encoding된 정답 label이다. \n",
    "\n",
    "여기선 아직 n으로 나눈 (평균) 것을 안한 듯 하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차\n",
    "\n",
    "cross entropy error, CEE\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/99C0D73B5A92769625\">\n",
    "\n",
    "tk는 정답일 때만 1인 one-hot encoding되어있는 것이기 때문에 실질적으론 정답일 때의 추정의 자연로그를 계산하는 식.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
