{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 학습 관련 기술들 \n",
    "\n",
    "이번 장에서 다룰 주제들:\n",
    "- 가중치 매개변수의 최적값을 탐색하는 최적화 방법\n",
    "- 가중치 매개변수 초깃값\n",
    "- 하이퍼파라미터 설정 방법\n",
    "- 오버피팅의 대응책: 정규화. (가중치 감소와 드롭아웃 등)\n",
    "\n",
    "## 6.1 매개변수 갱신\n",
    "\n",
    "손실함수의 값을 최소화하는 매개변수 최적 값을 찾는 것을 Optimization이라고 한다. \n",
    "\n",
    "지금까지 본 것은 기울기를 통해 이를 찾는 SGD 방법이었다. 이외의 방법도 알아보자. \n",
    "\n",
    "### 6.1.1 모험가 이야기\n",
    "\n",
    "최적화 상황은 눈 감고 지도 없이 가장 낮은 골짜기를 찾는 모험가와 같다. \n",
    " \n",
    "SGD는 발바닥으로 기울기를 느껴 가장 크게 기울어진 방향으로 내려가는 것이다. \n",
    "\n",
    "### 6.1.2 확률적 경사 하강법 (SGD)\n",
    "\n",
    "SGD는 수식으로\n",
    "\n",
    "![SGD](../deep_learning_images/e_6.1.png \"SGD equation\")\n",
    "\n",
    "이다. \n",
    "\n",
    "이를 구현해보면, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 짤 때 이런 식으로 optimizer 여러 개를 가지고 \n",
    "\n",
    "optimizer = SGD() \n",
    "\n",
    "와 같이 변수로 지정해주고 \n",
    "\n",
    "optimizer.update(params, grads) 와 같은 식으로 돌린다. (여러 optimizer로 바꿔가며 할 수 있음.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 SGD의 단점\n",
    "\n",
    "[fig6-1](../deep_learning_images/fig_6-1.png \"f(x,y) = (1/20)*x^2 + y^2의 그래프\")\n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-1.png\" width=\"500\">\n",
    "\n",
    "이는 A4용지를 구부린 뒤 중앙을 살짝 당긴 모양이다. (조금밖에 차이 안나지만 정 중앙이 최저점이긴 함.) \n",
    "\n",
    "와 같은 경우 기울기를 나타내면 \n",
    "\n",
    "[fig6-2](../deep_learning_images/fig_6-2.png \"기울기\") \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-2.png\" width=\"500\">\n",
    "\n",
    "\n",
    "와 같이 되고, 여기에 SGD를 적용하면 \n",
    "\n",
    "[fig6-3](../deep_learning_images/fig_6-3.png \"최적화 갱신 경로\") \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-3.png\" width=\"500\">\n",
    "\n",
    "이렇게 최적화 갱신 경로가 비효율적으로 나타난다. \n",
    "\n",
    "\n",
    "여기서 볼 수 있듯, SGD는 비등방성(antsotropy, 방향에 따라 성질이(여기서는 기울기) 달라지는 함수)에서 탐색 경로가 비효율적이다. \n",
    "\n",
    "즉, 기울기의 방향이 최솟값과 다른 방향이기 때문에 이런 문제가 발생하는 것이다. \n",
    "\n",
    "이에 대한 대안으로 \n",
    "- Momentum\n",
    "- AdaGrad\n",
    "- Adam\n",
    "\n",
    "을 소개한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 모멘텀\n",
    "\n",
    "수식으론 다음과 같다. \n",
    "\n",
    "![e6-3](../deep_learning_images/e_6.3.png \"e6.3\")\n",
    "\n",
    "![e6-4](../deep_learning_images/e_6.4.png \"e6.4\")\n",
    "\n",
    "W는 가중치, aL/aW는 손실함수의 기울기, n(에타)는 학습률, v는 물리학에서의 속도에 해당. a(알파)는 물리학에서의 지면 마찰이나 공지 저항에 해당. 보통 0.9 정도로 설정. \n",
    "\n",
    "av 항은 물체가 아무 힘도 받지 않을 때 서서히 하강시키는 역할이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모멘텀을 이용해 같은 함수에 대해 최적화를 진행하면 다음과 같이 된다. \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-5.png\" width=\"500\">\n",
    "\n",
    "왜 이렇게 되는가? \n",
    "\n",
    "x 축의 방향은 완만하지만 계속 내리막길이니 속도가 계속 붙고, y축의 방향은 가파르지만 골짜기처럼 왔다 갔다 할 때 기울기 +-가 계속 바뀌므로 속도가 늘었다 줄었다 한다. \n",
    "\n",
    "x 쪽은 갈수록 속도가 붙게 되고, SGD보다 덜 지그재그가 된다. \n",
    "\n",
    "### 6.1.5 AdaGrad \n",
    "\n",
    "학습률 n(에타)는 중요하다. 이를 잘 정하는 기술로 학습률 감소(learning rate decay)가 있다. 학습하며 학습률을 점점 줄여나가는 방법이다. \n",
    "\n",
    "AdaGrad는 각각의 매개변수에 적응적으로(adaptive) 학습률을 조정해준다. (매개변수 전체에 대해 공통으로 학습률 낮추는 것보다 나음.) \n",
    "\n",
    "수식으론 아래와 같다. \n",
    "\n",
    "![e6-5](../deep_learning_images/e_6.5.png \"e6.5\")\n",
    "\n",
    "![e6-6](../deep_learning_images/e_6.6.png \"e6.6\")\n",
    "\n",
    "h는 계속 기존 기울기를 제곱한 값을 더해가며 update된다. (o은 행렬의 원소별 곱셈) \n",
    "\n",
    "매개변수 갱신 시 1/sqrt(h)를 곱해 학습률을 조정한다. \n",
    "\n",
    "즉, 이는 매개변수의 원소 중 많이 움직인(크게 갱신된) 원소의 학습률을 낮춘다. \n",
    "\n",
    "<br>\n",
    "\n",
    "식을 살펴보면, h는 계속 기울기를 제곱한 값을 더해주며 갱신되기 때문에 항상 커진다. \n",
    "\n",
    "다만 기울기가 1보다 클 경우는 h가 크게 커지고, 1보다 작을 경우 h가 아주 작게 커지는 차이가 있다. \n",
    "\n",
    "<h1 style='color:purple'>AdaGrad라도 평탄한 구간을 지나면 학습률*갱신거의안되는h*작은기울기 니까 거의 W가 update 더 이상 못하고 확정되는거 아닌가? </h1>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "참고로, AdaGrad는 h에 과거 기울기를 제곱하며 더해가기 때문에 학습을 많이 진행하면 어느 순간 갱신량이 0이 되어 갱신되지 않게 된다. \n",
    "\n",
    "이를 해결하기 위한 방법으로 RMSProp이 있다. RMSProp은 과거의 모든 기울기를 균일하게 더하지 않고 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영한다. \n",
    "\n",
    "이는 지수이동평균(Exponential Moving Average, EMA)라고 하여 과거 기울기의 반영 규모를 기하급수적으로 감소시킨다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7) # 아주 작은 값 1e-7을 더해줘 설사 self.h[key]에 0이 담겨있어도 0으로 나누는 사태를 막아줌. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 통해 최적화 문제를 풀어보면 다음과 같이 나타난다. \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-6.png\" width=\"500\">\n",
    "\n",
    "y축 방향으로의 기울기가 커서 처음엔 크게 움직이지만(h가 크게 커짐), 큰 움직임 만큼 갱신 정도도 큰 폭으로 작아져(학습률에 1/sqrt(h)를 곱해주니까) y축 방향의 갱신강도가 빠르게 약해진다. \n",
    "\n",
    "### 6.1.6 Adam \n",
    "\n",
    "앞의 모멘텀과 AdaGrad를 융합한 것이 Adam이다. (엄밀하진 않은 설명)\n",
    "\n",
    "하이퍼파라미터의 편향보정이 진행된다는 점도 Adam의 특징이다. \n",
    "\n",
    "복잡해서 설명은 하지 않고, 구현은 ../common/optimizer.py 에 되어있다. \n",
    "\n",
    "결과는 아래와 같다. \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-7.png\" width=\"500\">\n",
    "\n",
    "직관적으로 보면 모멘텀과 같이 바닥 구르듯 부드럽게 움직이는데, 갱신 강도가 계속 약해지므로 좌우의(y축) 흔들림이 적다. \n",
    "\n",
    "참고로 Adam의 Hyperparameter는 3개이다. \n",
    "\n",
    "1. a (학습률)\n",
    "2. 1차 모멘텀 계수 B1, 논문에선 0.9\n",
    "3. 2차 모멘텀 계수 B2, 논문에선 0.999\n",
    "\n",
    "### 6.1.7 어느 갱신 방법을 이용할 것인가? \n",
    "\n",
    "결과를 비교해보자. \n",
    "\n",
    "여기선 AdaGrad가 가장 낫지만, 이는 푸는 문제에 따라 달라진다. \n",
    "\n",
    "많은 연구에선 SGD를 기본으로 쓰고, 요즘은 Adam도 많이 쓰인다. 알아서 여러 가지 실험해보며 맞는걸 고르자. \n",
    "\n",
    "### 6.1.8 MNIST 데이터셋으로 본 갱신방법 비교\n",
    "\n",
    "결과는 생략. 일반적으로 SGD보다 다른 기법이 빠르게 학습하고 loss function 최적화도 잘한다. \n",
    "\n",
    "## 6.2 가중치의 초깃값\n",
    "\n",
    "가중치의 초깃값이 중요하다. 권장 초기값에 대해 배우고 실습을 해보자. \n",
    "\n",
    "### 6.2.1 초깃값을 0으로 하면? \n",
    "\n",
    "가중치 감소(weight decay)는 오버피팅을 억제한다. \n",
    "\n",
    "하지만 만약 극단적으로 모두 0이면 어떻게 될까? 안된다. (정확히는, 모두 균일한 값으로 설정해선 안됨.)\n",
    "\n",
    "그 이유는 backpropagation 시 모든 가중치 값이 똑같이 갱신되기 때문. \n",
    "\n",
    "순전파시 모든 W가 같은 값이면 다음 층 뉴런도 모두 같은 값을 가지게 되고, 역전파에서도 덧셈은 말할 것도 없고 곱셈노드의 역전파를 생각하면 aL/aW에 같은 값이 곱해지기에 갱신을 해도 W에 변화가 없게 된다.\n",
    "\n",
    "이렇게 되면 가중치를 여러 개 갖는 의미가 없어진다. \n",
    "\n",
    "즉, 가중치의 대칭적 구조를 무너뜨리려면 초깃값을 무작위로 설정해야 한다. \n",
    "\n",
    "### 6.2.2 은닉층의 활성화값 분포\n",
    "\n",
    "가중치의 초깃값에 따라 은닉층 활성화 값들이 어떻게 변화하는지 살펴보자. (CS231n을 참고한 Sigmoid를 activation function으로 사용하는 5층 신경망)\n",
    "\n",
    "실험 전체 소스 코드는 weight_init_activation_histogram.py에 있다. \n",
    "\n",
    "일단 가중치 w = np.random.randn(node_num, node_num) * 1(표준편차) 으로 뽑았는데, 이 표준편차를 바꿔가며 활성화 값들의 분포의 변화를 관찰하는 것이 실험의 목표이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.random.randn(1000, 100) #1000개의 데이터 중 100개 뽑음. \n",
    "node_num = 100 # 각 은닉층의 노드(뉴런) 수\n",
    "hidden_layer_size = 5 # 은닉층이 5개. \n",
    "activations = {} # 여기에 활성화 결과(활성화 값)을 저장. \n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i !=  0:\n",
    "        x = activations[i-1]\n",
    "        \n",
    "    w = np.random.randn(node_num, node_num) * sd\n",
    "#     w = np.random.randn(node_num, node_num) / np.sqrt(node_num) # Xavier 초깃값\n",
    "    a = np.dot(x, w)\n",
    "    z = sigmoid(a)\n",
    "    \n",
    "    activations[i] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 히스토그램 그리기 \n",
    "\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "볼 수 있듯, 활성화 값들이 0과 1에 치우쳐 분포되어 있다. \n",
    "\n",
    "시그모이드 함수는 아래와 같이 생겼기 때문에, \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_3-8.png\" width=\"400\">\n",
    "\n",
    "지금처럼 출력이 0 또는 1에 가까워지면 미분값이 0으로 수렴한다. 그러면 역전파의 기울기 값이 점점 작아지다가 사라지게 된다. (기울기 소실, gradient vanishing) \n",
    "\n",
    "기울기 소실은 층이 깊어지는 딥러닝에서 큰 문제가 될 수 있다. \n",
    "\n",
    "이번엔 똑같이 하되 sd를 0.01로 해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = 0.01 # sd를 설정했으니 위의 코드블럭을 그대로 돌린다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번엔 반대로 0.5 부근에 집중됨을 볼 수 있다. 기울기 소실은 일어나지 않지만, 활성화 값이 치우쳤기 때문에 표현력을 제한한다. 여러 뉴런을 둔 것이 1개 두는 것 과 별반 다를게 없어지는 것이다. \n",
    "\n",
    "따라서, 제대로 된 초깃값을 설정해 주어야 한다. \n",
    "\n",
    "Xavier Glorot와 Yoshua Bengio의 논문에서 권장하는 Xavier 초깃값을 써보자. (일반적인 딥러닝 프레임워크에서 표준으로 사용됨.)\n",
    "\n",
    "논문의 결론만 말하자면, 활성화 값들을 광범위하게 분포시키려면 노드의 갯수를 n이라 할 때 sd = 1/sqrt(n) = sqrt(1/n) 인 분포를 사용하면 된다. \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-12.png\" width=\"500\">\n",
    "\n",
    "즉, 앞 층의 노드가 많을수록 sd가 작아지므로 초깃값 가중치가 좁게 퍼진다. \n",
    "\n",
    "위의 코드에서 주석처리되어있는 Xavier 초깃값 라인의 주석처리를 풀어 돌려보자. \n",
    "\n",
    "그 결과는 층이 깊어지면서 형태가 다소 일그러지기는 하지만 얕은 층에서 적당히 잘 퍼진 모습을 보여주고 앞의 방식보다 나음을 확인할 수 있다. \n",
    "\n",
    "<br>\n",
    "\n",
    "참고로, Xavier을 써도 층이 깊어지면 히스토그램이 일그러지는 것은 sigmod함수 대신 tanh(쌍곡선 함수)를 이용하면 개선된다. \n",
    "\n",
    "Sigmoid와 거의 같은 모양이지만 (0,0)에서 대칭인 대신 (0,0.5)에서 대칭인 S자형 곡선이다. \n",
    "\n",
    "하지만 활성화 함수용으로는 원점에서 대칭인 함수가 바람직하다고 알려져 있다. \n",
    "\n",
    "### 6.2.3 ReLU를 사용할 때의 가중치 초깃값 \n",
    "\n",
    "Xavier 함수는 활성화 함수가 선형인 것을 전제로 이끈 결과이다. (Sigmoid나 tanh도 좌우 대칭이라 중앙 부근에선 선형이라 볼 수 있음.) \n",
    "\n",
    "하지만 ReLU는 여기에 맞는 초깃값을 사용해줘야 함. (Kalming He가 발견한 He 초깃값.)\n",
    "\n",
    "He 초깃값은 앞 게층의 노드를 n이라 할 때 sd = sqrt(2/n) 인 정규분포를 사용함. 이는 Xavier sd의 2배임.  \n",
    "\n",
    "활성화 함수가 Relu 일 때의 Xavier 초깃값, He 초깃값을 비교해보자. \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-14.png\" width=\"500\">\n",
    "\n",
    "1. 1번 경우는 각 층의 활성화 값들이 매우 작은 값이다. 신경망에 아주 작은 데이터가 흐른다는 것은 역전파 시 가중치의 기울기 역시 작아짐을 뜻하며, 학습이 거의 이뤄지지 않게 된다. \n",
    "2. Xavier을 보면 층이 깊어지며 점점 치우친다. 자꾸 0에 치우치므로 기울기 소실 문제를 불러오고, 표현력 문제도 생긴다. \n",
    "3. He 초깃값은 모든 층에서 균일하게 분포되었다. \n",
    "\n",
    "### 6.2.4 MNIST 데이터셋으로 본 가중치 초깃값 비교\n",
    "\n",
    "결과적으로 실제 데이터에서도 sd = 0.01로 단순히 놓고 초깃값을 설정하면 거의 학습이 안되고, Xavier 또는 He를 써야 한다. \n",
    "\n",
    "ReLU 활성화 함수인 경우는 He 초깃값을 쓰는 것이 낫다. \n",
    "\n",
    "## 6.3 배치 정규화 \n",
    "\n",
    "배치 정규화는(Batch Normalization) '각 층이 활성화를 적당히 퍼뜨리도록 강제해보면 어떨까?' 라는 아이디어에서 출발했다. \n",
    "\n",
    "### 6.3.1 배치 정규화 알고리즘 \n",
    "\n",
    "2015년에 제안됨. 장점은, \n",
    "\n",
    "- 학습을 빨리 진행할 수 있다. (학습 속도 개선)\n",
    "- 초깃값에 크게 의존하지 않는다.\n",
    "- 오버피팅을 억제한다. (드롭아웃 등의 필요성 감소)\n",
    "\n",
    "각 층에서 활성화 값이 적당히 분포되도록 하기 위해 배치 정규화 (Batch norm) 계층을 신경망에 삽입한다. \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-16.png\" width=\"500\">\n",
    "\n",
    "배치 정규화는 학습 시 미니배치를 단위로 정규화한다. 데이터 분포 ~ N(0,1) 가 되도록 만든다. \n",
    "\n",
    "![e6.7](../deep_learning_images/e_6.7.png \"e6.7\")\n",
    "\n",
    "수식은 복잡할 것이 없다. 미니배치 B = {x1, x2, ..., xm} 에 대해 평균과 분산을 구하고 정규분포로 만들어준다. 분모의 E(입실론)는 매우 작은 수로 0으로 나누는 사태를 방지한다. \n",
    "\n",
    "<br>\n",
    "\n",
    "그 다음은 배치 정규화 계층마다 정규화된 데이터에 고유한 확대(scale)와 이동(shift) 변환을 수행한다. \n",
    "\n",
    "이는 수식으로 아래와 같다. \n",
    "\n",
    "![e6.8](../deep_learning_images/e_6.8.png \"e6.8\")\n",
    "\n",
    "감마는 확대(1부터 시작. 1배 확대), B는 이동을 담당한다. (0부터 시작. 0만큼 이동(그대로))\n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-17.png\" width=\"600\">\n",
    "\n",
    "자세한 설명은 <a href=\"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\">Frederik Kratzert의 블로그</a>에서 확인 가능하다. \n",
    "\n",
    "### 6.3.2 배치 정규화의 효과\n",
    "\n",
    "MNIST 데이터를 이용해 배치 정규화 계층을 사용했을 때와 안했을 때를 비교해 보자. \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-18.png\" width=\"600\">\n",
    "\n",
    "배치 정규화를 하지 않으면 초깃값이 잘 분포되어있지 않을 경우 학습이 전혀 진행되지 않는 경우도 있다. \n",
    "\n",
    "## 6.4 바른 학습을 위해 \n",
    "\n",
    "### 6.4.1 오버피팅\n",
    "\n",
    "오버피팅은 주로 다음 두 경우에 일어난다. \n",
    "\n",
    "1. 매개변수가 많고 표현력이 높은 모델\n",
    "2. 훈련 데이터가 적음. \n",
    "\n",
    "코드는 overfit_weight_decay.py 를 참고. \n",
    "\n",
    "이 경우 다음과 같이 training data에 대한 결과는 epoch별로 곧 100% accuracy를 보이지만 test data의 결과는 크게 못 미치는 것을 확인할 수 있다. \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-20.png\" width=\"500\">\n",
    "\n",
    "### 6.4.2 가중치 감소\n",
    "\n",
    "weight decay. 큰 가중치에 대해 큰 페널티를 부과하여 오버피팅을 억제하는 방법. (오버피팅은 가중치가 커서 발생하는 경우가 많음.)\n",
    "\n",
    "<h1 style='color:purple'>잘 이해 안되는 부분... norm? pg.217~218</h1>\n",
    "\n",
    "신경망 학습의 목표인 손실함수의 값을 줄이기 위해 가중치의 제곱norm(L2 norm)을 손실함수에 더한다. 이를 통해 가중치가 커지는 것을 억제한다. \n",
    "\n",
    "L2norm이란 각 원소의 제곱들을 더한 것을 뜻한다. ?????????????????????????????????\n",
    "\n",
    "가중치를 W라 하면 L2norm에 따른 가중치 감소는 (1/2)*람다*W^2가 되고 이 값을 손실함수에 더한다. (람다는 정규화의 세기를 조절하는 hyperparameter. 클수록 큰 가중치에 대한 패널티가 커짐.)\n",
    "\n",
    "결과적으로 accuracy를 계산해보면 train data도 이제 100%를 기록하지 못하고, test와 train 사이의 간격은 줄었다. (overfitting이 억제됨.)\n",
    "\n",
    "### 6.4.3 드롭아웃\n",
    "\n",
    "가중치 감소는 간단히 구현할 수 있고, 어느 정도 overfitting을 억제할 수 있지만 NN 모델이 복잡해지면 다른 대응책도 필요하다. \n",
    "\n",
    "드롭아웃(Dropout)은 뉴런을 임의로 삭제하면서 학습하는 방법이다. 훈련 때 은닉층의 뉴런을 무작위로 골라 삭제한다. \n",
    "\n",
    "시험 때는 모든 뉴런에 신호를 전달한다. (단, 각 뉴런의 출력에 훈련 때 삭제한 비율을 곱하여 출력함.) \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-22.png\" width=\"500\">\n",
    "\n",
    "<h1 style='color:purple'>이 부분도 설명 부족함.</h1>\n",
    "\n",
    "드롭아웃을 구현해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio # 순전파 때 마다 x와 shape가 같은 배열을 무작위 생성하여 삭제할 뉴런을 False로 표시. \n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout): # 순전파 때 통과한 뉴런은 역전파 때도 그대로 통과함. \n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 효과를 MNIST 데이터 셋으로 확인하면 \n",
    "\n",
    "<img src=\"../deep_learning_images/fig_6-23.png\" width=\"500\">\n",
    "\n",
    "왼쪽은 드랍아웃 없이, 오른쪽은 드랍아웃을 넣어 구현한 것이다. \n",
    "\n",
    "- train data 정확도가 100%에 도달하지 않게 됨. \n",
    "- test data와 train data 사이의 정확도 차이가 줄어듦. \n",
    "\n",
    "참고로 Dropout은 Ensemble Learning와 관련이 깊다. 앙상블 학습은 개별적으로 학습시킨 여러 모델의 출력을 평균내여(또는 투표하여) 추론하는 방식인데, 신경망의 관점에선 비슷한 구조의 네트워크를 5개 준비해 따로 학습시키고 출력을 평균내는 것과 같다. \n",
    "\n",
    "드롭아웃이 매번 뉴런을 무작위로 삭제하는 행위를 매번 다른 네트워크로 학습시키는 것으로 해석할 수 있기 때문에 비슷한 역할을 한다고 볼 수 있다. \n",
    "\n",
    "## 6.5 적절한 하이퍼파라미터 값 찾기. \n",
    "\n",
    "각 층의 뉴런 수, 배치 크기, 매개변수 갱신 시의 학습률, 가중치 감소 등이 하이퍼 파라미터에 속한다. 적절한 설정이 중요하다. 최대한 효율적으로 탐색하는 법을 알아보자. \n",
    "\n",
    "### 6.5.1 검증 데이터\n",
    "\n",
    "하이퍼 파라미터는 성능 평가할 때 test data로 하면 안된다. (test data에 적합하도록 조정되어버리기 때문에.)\n",
    "\n",
    "hyperparameter 전용 확인 데이터가 필요하다. 이를 보통 검증 데이터(Validation data)라 부른다. \n",
    "\n",
    "- 훈련 데이터: 매개변수 학습\n",
    "- 검증 데이터: hyperparameter 성능 평가 \n",
    "- 시험 데이터: 신경망의 범용 성능 평가 \n",
    "\n",
    "보통 train data의 20% 정도를 validation data로 분리한다. \n",
    "\n",
    "train data의 상태에 따라 한 번 shuffle 한 다음 validation data를 꺼내기도 한다. (순서대로 정렬되어 있는 경우 등)\n",
    "\n",
    "### 6.5.2 하이퍼파라미터 최적화\n",
    "\n",
    "하이퍼파라미터 최적화의 핵심은 '최적 값'이 존재하는 범위를 조금씩 줄여나가는 것이다. \n",
    "\n",
    "대략적인 범위를 설정하고 그 범위에서 무작위로 hyperparameter 값을 sampling하고 그 값으로 정확도를 평가한다. 이 작업을 반복하며 범위를 좁혀나간다. \n",
    "\n",
    "신경망의 하이퍼파라미터 최적화는 grid search같은 규칙적 탐색보단 무작위 sampling이 더 좋은 결과를 낸다고 알려져있다. (최종 정확도에 미치는 영향이 하이퍼파라미터마다 다르기 때문.)\n",
    "\n",
    "대략적인 범위는 보통 10의 거듭제곱 단위로 지정한다. (log scale 지정) 예를 들어 10^-3 ~ 10^3 와 같은 식이다. \n",
    "\n",
    "하이퍼파라미터 최적화는 computing power가 많이 들어 매우 오래 걸리므로 학습 epoch을 작게 하여 1회 평가에 걸리는 시간을 단축하는 것이 좋다. \n",
    "\n",
    "1. 하이퍼파라미터 값의 범위를 설정한다. \n",
    "2. 설정된 범위에서 하이퍼파라미터 값을 무작위 추출한다. \n",
    "3. 샘플링한 하이퍼파라미터 값을 사용해 학습하고 검증 데이터로 정확도를 평가. (epoch은 작게 설정)\n",
    "4. 2,3을 특정 횟수 반복하며 하이퍼파라미터의 범위를 좁힘. \n",
    "\n",
    "추가적으로 더 세련된 기법으로 베이즈 최적화(Bayesian Optimization)를 쓸 수 있다. \n",
    "\n",
    "### 6.5.3 하이퍼파라미터 최적화 구현하기. \n",
    "\n",
    "weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "lr = 10 ** np.random.uniform(-6, -2) \n",
    "\n",
    "와 같은 식으로 무작위 추출하여 시행한다. \n",
    "\n",
    "잘 되는 값의 범위를 관찰하고 범위를 좁혀간다. \n",
    "\n",
    "## 6.6 정리\n",
    "\n",
    "- 매개변수 갱신 방법에는 SGD외에도 모멘텀, AdaGrad, Adam 등이 있다. \n",
    "- 가중치 초깃값을 정하는 방법은 올바른 학습을 위해 매우 중요하다. \n",
    "- 그 방법으론 Xavier 초깃값과 He 초깃값(ReLU 전용)이 효과적이다. \n",
    "- 배치 정규화를 이용하면 학습을 빠르게 진행할 수 있으며, 초깃값에 영향을 덜 받게 된다. \n",
    "- 오버피팅을 억제하는 정규화 기술로는 가중치 감소와 드롭아웃이 있다. \n",
    "- 하이퍼파라미터 값 탐색은 최적 값이 존재할 법한 범위를 점차 좁히면서 하는 것이 효과적이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
